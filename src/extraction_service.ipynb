{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0a0f948-8bfd-4a0d-bf01-0fca5998ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pdf_utils import PDFTextAndImageExtractor\n",
    "from log_utils import ExtractionLogManager\n",
    "from azure_doc_intelligence import AzureDocIntelligenceRunner\n",
    "from openai_utils import AzureResponsesClient\n",
    "from agent_prompts import human_reviewed_schema, table_grid_emptiness, extractor_with_feedback, judge, extractor, OCR_content_filter, coordinate_based_empty_cell_finder\n",
    "import itertools\n",
    "\n",
    "def prepare_table_feedback(issues):\n",
    "    return '\\n'.join([f'''Issue {i+1}:\n",
    "row: {issue['row']}\n",
    "column: {issue['column']}\n",
    "{issue['feedback']}\n",
    "-------------''' for i,issue in enumerate(issues)])\n",
    "\n",
    "def iter_dict_chunks(d, n):\n",
    "    it = iter(d.items())\n",
    "    while True:\n",
    "        chunk_items = list(itertools.islice(it, n))\n",
    "        if not chunk_items:\n",
    "            break\n",
    "        yield dict(chunk_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e2bed03-b784-488b-8519-2ec0076892be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pages: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:39<00:00,  3.97s/it]\n"
     ]
    }
   ],
   "source": [
    "pdf_extractor = PDFTextAndImageExtractor()\n",
    "# Extract PIL image, full text, bytes, base64 image from a PDF file\n",
    "per_page_text_and_images = pdf_extractor.process_pdf('obfuscated_fake_cbiz_prof_10_pages.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df420afd-4dc4-4c27-9723-464ba529951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- new import near other imports ---\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ... existing imports and helpers ...\n",
    "# import logging, json, pandas as pd, Path, PDFTextAndImageExtractor, ExtractionLogManager, AzureDocIntelligenceRunner, AzureResponsesClient, agent prompts, itertools ...\n",
    "\n",
    "# add near other imports\n",
    "import re\n",
    "import ast\n",
    "\n",
    "def _strip_code_fences(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove a single fenced Markdown code block if present, preserving inner text.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return s\n",
    "    m = re.search(r\"``````\", s)\n",
    "    return m.group(1) if m else s\n",
    "\n",
    "def _extract_first_bracket_block(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the first balanced {...} or [...] block to ignore preambles/epilogues.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return s\n",
    "    start_obj = s.find(\"{\")\n",
    "    start_arr = s.find(\"[\")\n",
    "    starts = [i for i in (start_obj, start_arr) if i != -1]\n",
    "    if not starts:\n",
    "        return s.strip()\n",
    "    start = min(starts)\n",
    "    stack = []\n",
    "    for i, ch in enumerate(s[start:], start):\n",
    "        if ch in \"{[\":\n",
    "            stack.append(ch)\n",
    "        elif ch in \"}]\":\n",
    "            if not stack:\n",
    "                break\n",
    "            opener = stack.pop()\n",
    "            if (opener == \"{\" and ch != \"}\") or (opener == \"[\" and ch != \"]\"):\n",
    "                # mismatched but keep scanning\n",
    "                pass\n",
    "            if not stack:\n",
    "                return s[start:i + 1]\n",
    "    return s[start:].strip()\n",
    "\n",
    "def parse_llm_json(text: str):\n",
    "    \"\"\"\n",
    "    Robustly parse LLM 'JSON' that may be:\n",
    "    - fenced in ``````\n",
    "    - prefixed/suffixed with prose\n",
    "    - single-quoted Python-literals\n",
    "\n",
    "    Strategy: strip fences -> extract first balanced block -> json.loads -> ast.literal_eval.\n",
    "    \"\"\"\n",
    "    raw = _strip_code_fences(text or \"\").strip()\n",
    "    raw = _extract_first_bracket_block(raw)\n",
    "    # Try strict JSON first\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback to safe Python literal parsing\n",
    "    try:\n",
    "        return ast.literal_eval(raw)\n",
    "    except Exception:\n",
    "        logging.getLogger(\"extract\").exception(\"Failed to parse LLM output as JSON/Python literal\")\n",
    "        # Best-effort neutral default to keep pipeline moving\n",
    "        return {} if raw.startswith(\"{\") else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8afebcf1-39ab-460e-b94f-498ec8d8073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_page(page_idx1: int,\n",
    "                     page_data: dict,\n",
    "                     extraction_id: str,\n",
    "                     root: Path,\n",
    "                     judge_retry_max_attempts: int) -> int:\n",
    "    \"\"\"\n",
    "    Process a single page (1-based index) in its own process:\n",
    "    - sets up per-page logger\n",
    "    - runs initial extraction\n",
    "    - runs judge / correction loops for form fields and tables\n",
    "    - writes original_extraction.json and corrected_extraction.json\n",
    "    \"\"\"\n",
    "    # Recreate a logger manager in this worker process\n",
    "    mgr = ExtractionLogManager(\n",
    "        base_dir=Path(\"./runs\"),\n",
    "        parent_logger_name=\"extract\",\n",
    "        level=logging.INFO,\n",
    "        enable_console=True,\n",
    "        console_format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    )\n",
    "\n",
    "    with mgr.page_logger(root, extraction_id, page_num=page_idx1) as (logger, page_dir):\n",
    "        logger.info('started initial extraction for page %s', page_idx1)\n",
    "\n",
    "        openai_client = AzureResponsesClient(\n",
    "            model=extractor['model'],\n",
    "            extraction_id=extraction_id,\n",
    "            page_num=page_idx1,\n",
    "        )\n",
    "\n",
    "        # minimal page payload expected: full_text, base64_image, page_bytes\n",
    "        initial_extraction_response = openai_client.invoke(\n",
    "            page_data['full_text'],\n",
    "            page_data['base64_image'],\n",
    "            system_prompt=extractor['system_prompt'],\n",
    "            user_prompt=extractor['user_prompt'],\n",
    "            reasoning_effort=extractor['reasoning_effort'],\n",
    "            model=extractor['model'],\n",
    "            data_type='FULL_RAW_TEXT',\n",
    "        )\n",
    "\n",
    "        # safer than eval\n",
    "        initial_text = openai_client.extract_response_text(initial_extraction_response) or \"{}\"\n",
    "        extracted_data = parse_llm_json(initial_text)\n",
    "\n",
    "        logger.info('initial extraction of page data done.')\n",
    "        with (page_dir / \"original_extraction.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(extracted_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        # ==== PROCESSING EACH PAGE'S EXTRACTED DATA ====\n",
    "        for data_part in ['Tables', 'Form_fields']: # process tables first, since they will be corrected using OCR\n",
    "            if data_part == 'Form_fields':  # should match human_reviewed_schema key\n",
    "                logger.info('started validating form fields')\n",
    "\n",
    "                # processing 10 form fields at a time\n",
    "                for field_subset in iter_dict_chunks(extracted_data[data_part], 10):\n",
    "                    field_list = list(field_subset.keys())\n",
    "                    logger.info('-validating form fields: %s', field_list)\n",
    "                    logger.info('--current are:\\n%s', '\\n'.join([k + ' : ' + (v or '') for k, v in field_subset.items()]))\n",
    "\n",
    "                    # passing processed tables to distinguish between form fields and table better when they mixed up badly\n",
    "                    ptable = ['## Page Tables Start']\n",
    "                    for name, trows in (extracted_data.get('Tables') or {}).items():\n",
    "                        df = pd.DataFrame(trows)\n",
    "                        ptable.append(f\"\\n### {name}\")\n",
    "                        ptable.append(df.to_markdown(index=False) if not df.empty else '_(empty_)')\n",
    "                    ptable.append('## Page Tables End')\n",
    "                    page_table_mrk = '\\n'.join(ptable)\n",
    "                    \n",
    "                    judge_retry_attempt = 1\n",
    "                    while judge_retry_attempt <= judge_retry_max_attempts:\n",
    "                        if judge_retry_attempt > 1:\n",
    "                            logger.info('-revalidating form fields: %s', field_list)\n",
    "\n",
    "                        judge_agent = judge.copy()\n",
    "                        judge_agent['user_prompt'] = (judge_agent['user_prompt']\n",
    "                            .replace('DATA_PART_INSTRUCTIONS', judge_agent['form_field_instructions'])\n",
    "                            .replace('DATA_PART', data_part)\n",
    "                            .replace('DATA_NAME', '')\n",
    "                            .replace('DATA_VALUE', '\\n'.join([k + ':' + (v or '') for k, v in field_subset.items()]))\n",
    "                            .replace('## GRID_INFO:', page_table_mrk)\n",
    "                            .replace('OUTPUT_FORMAT', judge_agent['form_field_output_format'])\n",
    "                        )\n",
    "\n",
    "                        judge_response = openai_client.invoke(\n",
    "                            page_data['full_text'],\n",
    "                            page_data['base64_image'],\n",
    "                            system_prompt=judge_agent['system_prompt'],\n",
    "                            user_prompt=judge_agent['user_prompt'],\n",
    "                            reasoning_effort=judge_agent['reasoning_effort'],\n",
    "                            model=judge_agent['model'],\n",
    "                            data_type='FULL_RAW_TEXT',\n",
    "                        )\n",
    "\n",
    "                        form_field_text = openai_client.extract_response_text(judge_response) or \"[]\"\n",
    "                        form_field_feedback = parse_llm_json(form_field_text)\n",
    "                        form_field_issues = [ff for ff in form_field_feedback if ff.get('status') == 'wrong']\n",
    "\n",
    "                        if form_field_issues:\n",
    "                            logger.info('--found issues: %s', form_field_issues)\n",
    "                            judge_retry_attempt += 1\n",
    "\n",
    "                            form_field_feedback_string = '\\n'.join(\n",
    "                                [ffi['data_name'] + ':' + ffi.get('feedback', '') for ffi in form_field_issues]\n",
    "                            )\n",
    "\n",
    "                            extractor_with_feedback_agent = extractor_with_feedback.copy()\n",
    "                            extractor_with_feedback_agent['user_prompt'] = (extractor_with_feedback_agent['user_prompt']\n",
    "                                .replace('DATA_PART', data_part)\n",
    "                                .replace('DATA_VALUE', '\\n'.join([k + ':' + (v or '') for k, v in field_subset.items()]))\n",
    "                                .replace('FEEDBACK', form_field_feedback_string)\n",
    "                                .replace('OUTPUT_FORMAT', extractor_with_feedback_agent['form_field_output_format'])\n",
    "                                .replace('DATA_NAME', '')\n",
    "                            )\n",
    "\n",
    "                            extractor_with_feedback_response = openai_client.invoke(\n",
    "                                page_data['full_text'],\n",
    "                                page_data['base64_image'],\n",
    "                                system_prompt=extractor_with_feedback_agent['system_prompt'],\n",
    "                                user_prompt=extractor_with_feedback_agent['user_prompt'],\n",
    "                                reasoning_effort=extractor_with_feedback_agent['reasoning_effort'],\n",
    "                                model=extractor_with_feedback_agent['model'],\n",
    "                                data_type='FULL_RAW_TEXT',\n",
    "                            )\n",
    "\n",
    "                            correction_text = openai_client.extract_response_text(extractor_with_feedback_response) or \"{}\"\n",
    "                            extracted_data_correction = parse_llm_json(correction_text)\n",
    "\n",
    "                            logger.info('--corrected as:\\n%s', '\\n'.join([k + ' : ' + (v or '') for k, v in extracted_data_correction.items()]))\n",
    "\n",
    "                            field_subset.update(extracted_data_correction)\n",
    "                        else:\n",
    "                            logger.info('done judging, ALL GOOD')\n",
    "                            break\n",
    "\n",
    "                    extracted_data[data_part].update(field_subset)\n",
    "\n",
    "            if data_part == 'Tables':\n",
    "                # get markdown from Azure Document Intelligence only for tables\n",
    "                runner = AzureDocIntelligenceRunner(\n",
    "                    model_id=\"prebuilt-layout\",\n",
    "                    parent_logger_name=\"extract\",\n",
    "                    extraction_id=extraction_id,\n",
    "                    page_num=page_idx1,\n",
    "                )\n",
    "\n",
    "                # markdown_str = runner.analyze_page_to_markdown(page_data['page_bytes'])\n",
    "                azure_doc_res = runner.analyze_page(page_data['page_bytes'])\n",
    "                relavent_ocr_info = []\n",
    "                for element in azure_doc_res['pages'][0]['lines']:\n",
    "                    _ = element.pop('spans', 'Key not found')\n",
    "                    relavent_ocr_info.append(element)\n",
    "                \n",
    "                logger.info('got response with OCR coordinates info from azure doc intelligence for page %s', page_idx1)\n",
    "                logger.info('started validating tables')\n",
    "\n",
    "                # processing 1 table at a time\n",
    "                for table in iter_dict_chunks(extracted_data[data_part], 1):\n",
    "                    table_name, table_data = table.popitem()\n",
    "                    df = pd.DataFrame(table_data)\n",
    "                    table_mrk_str = df.to_markdown(index=False)\n",
    "                    logger.info('-current table:\\n%s', table_mrk_str)\n",
    "                    logger.info('-validating table: %s', table_name)\n",
    "\n",
    "                    if not table_data:\n",
    "                        continue  # if table is empty do not check\n",
    "\n",
    "                    logger.info('--figuring out table emptiness...')\n",
    "                    # grid_emptiness_agent = table_grid_emptiness.copy()\n",
    "                    # grid_emptiness_agent['user_prompt'] = (grid_emptiness_agent['user_prompt']\n",
    "                    #     .replace('TABLE_NAME', table_name)\n",
    "                    #     .replace('TABLE_COLUMNS', str(human_reviewed_schema[data_part][table_name]))\n",
    "                    # )\n",
    "\n",
    "                    # grid_emptiness_response = openai_client.invoke(\n",
    "                    #     markdown_str,\n",
    "                    #     page_data['base64_image'],\n",
    "                    #     system_prompt=grid_emptiness_agent['system_prompt'],\n",
    "                    #     user_prompt=grid_emptiness_agent['user_prompt'],\n",
    "                    #     reasoning_effort=grid_emptiness_agent['reasoning_effort'],\n",
    "                    #     model=grid_emptiness_agent['model'],\n",
    "                    #     data_type='MARKDOWN',\n",
    "                    # )\n",
    "                    # grid_emptiness_info = openai_client.extract_response_text(grid_emptiness_response) or \"\"\n",
    "\n",
    "                    ocr_agent = OCR_content_filter.copy()\n",
    "                    ocr_agent['user_prompt'] = (ocr_agent['user_prompt']\n",
    "                        .replace('TABLE_NAME', table_name)\n",
    "                        .replace('TABLE_CONTENT', table_mrk_str)\n",
    "                    )\n",
    "\n",
    "                    table_relate_filtered_ocr_response = openai_client.invoke(\n",
    "                        str(relavent_ocr_info),\n",
    "                        page_data['base64_image'],\n",
    "                        system_prompt=ocr_agent['system_prompt'],\n",
    "                        user_prompt=ocr_agent['user_prompt'],\n",
    "                        reasoning_effort=ocr_agent['reasoning_effort'],\n",
    "                        model=ocr_agent['model'],\n",
    "                        data_type='OCR_COORDINATES',\n",
    "                    )\n",
    "\n",
    "                    table_relate_filtered_ocr_info = openai_client.extract_response_text(table_relate_filtered_ocr_response) or \"\"\n",
    "                    logger.info('--got table related OCR content from Azure Doc Intelligence')\n",
    "\n",
    "                    table_grid_emptiness_agent = coordinate_based_empty_cell_finder.copy()\n",
    "                    table_grid_emptiness_agent['user_prompt'] = (table_grid_emptiness_agent['user_prompt']\n",
    "                        .replace('OCR_CONTENT', table_relate_filtered_ocr_info)\n",
    "                        .replace('TABLE_NAME', table_name)\n",
    "                        .replace('COLUMN_HEADERS', '|' + '|'.join(human_reviewed_schema[data_part][table_name]) + '|')\n",
    "                    )\n",
    "\n",
    "                    table_grid_emptiness_response = openai_client.invoke(\n",
    "                        None,\n",
    "                        None,\n",
    "                        system_prompt=table_grid_emptiness_agent['system_prompt'],\n",
    "                        user_prompt=table_grid_emptiness_agent['user_prompt'],\n",
    "                        reasoning_effort=table_grid_emptiness_agent['reasoning_effort'],\n",
    "                        model=table_grid_emptiness_agent['model'],\n",
    "                        data_type=None,\n",
    "                    )\n",
    "                    \n",
    "                    table_grid_emptiness_info = openai_client.extract_response_text(table_grid_emptiness_response) or \"\"\n",
    "                    logger.info('--got table grid emptiness info:\\n%s', table_grid_emptiness_info)\n",
    "\n",
    "                    judge_retry_attempt = 1\n",
    "                    while judge_retry_attempt <= judge_retry_max_attempts:\n",
    "                        if judge_retry_attempt > 1:\n",
    "                            logger.info('-revalidating table: %s', table_name)\n",
    "                            df = pd.DataFrame(table_data)\n",
    "                            table_mrk_str = df.to_markdown(index=False)\n",
    "\n",
    "                        judge_agent = judge.copy()\n",
    "\n",
    "                        judge_agent['user_prompt'] = (judge_agent['user_prompt']\n",
    "                            .replace('DATA_PART_INSTRUCTIONS', judge_agent['table_instructions'])\n",
    "                            .replace('DATA_PART', data_part)\n",
    "                            .replace('DATA_NAME', table_name)\n",
    "                            .replace('DATA_VALUE', table_mrk_str)\n",
    "                            .replace('## GRID_INFO:', f\"## GRID_INFO (tells whether a cell is empty or not, non empty cells are marked as 'X', trusted since calculated using OCR coordinates) :\\n{table_grid_emptiness_info}\\n\")\n",
    "                            .replace('OUTPUT_FORMAT', judge_agent['table_output_format'])\n",
    "                        )\n",
    "\n",
    "                        judge_response = openai_client.invoke(\n",
    "                            page_data['full_text'],\n",
    "                            page_data['base64_image'],\n",
    "                            system_prompt=judge_agent['system_prompt'],\n",
    "                            user_prompt=judge_agent['user_prompt'],\n",
    "                            reasoning_effort=judge_agent['reasoning_effort'],\n",
    "                            model=judge_agent['model'],\n",
    "                            data_type='FULL_RAW_TEXT',\n",
    "                        )\n",
    "\n",
    "                        tables_text = openai_client.extract_response_text(judge_response) or \"[]\"\n",
    "                        tables_feedback = parse_llm_json(tables_text)\n",
    "                        tables_issues = [t_issue for t_issue in tables_feedback if t_issue.get('status') == 'wrong']\n",
    "\n",
    "                        if tables_issues:\n",
    "                            logger.info('--found issues: %s', tables_issues)\n",
    "                            judge_retry_attempt += 1\n",
    "\n",
    "                            table_feedback_string = prepare_table_feedback(tables_issues)\n",
    "\n",
    "                            extractor_with_feedback_agent = extractor_with_feedback.copy()\n",
    "                            extractor_with_feedback_agent['user_prompt'] = (extractor_with_feedback_agent['user_prompt']\n",
    "                                .replace('DATA_PART', data_part)\n",
    "                                .replace('DATA_VALUE', table_mrk_str)\n",
    "                                .replace('FEEDBACK', table_feedback_string)\n",
    "                                .replace('OUTPUT_FORMAT', extractor_with_feedback_agent['table_output_format'])\n",
    "                                .replace('DATA_NAME', table_name)\n",
    "                            )\n",
    "\n",
    "                            extractor_with_feedback_response = openai_client.invoke(\n",
    "                                page_data['full_text'],\n",
    "                                page_data['base64_image'],\n",
    "                                system_prompt=extractor_with_feedback_agent['system_prompt'],\n",
    "                                user_prompt=extractor_with_feedback_agent['user_prompt'],\n",
    "                                reasoning_effort=extractor_with_feedback_agent['reasoning_effort'],\n",
    "                                model=extractor_with_feedback_agent['model'],\n",
    "                                data_type='FULL_RAW_TEXT',\n",
    "                            )\n",
    "\n",
    "                            correction_text = openai_client.extract_response_text(extractor_with_feedback_response) or \"[]\"\n",
    "                            extracted_data_correction = parse_llm_json(correction_text)\n",
    "\n",
    "                            logger.info('--corrected as:\\n%s', pd.DataFrame(extracted_data_correction).to_markdown(index=False))\n",
    "\n",
    "                            table_data = extracted_data_correction  # update old data with new corrected data\n",
    "                            table.update({table_name: table_data})\n",
    "                        else:\n",
    "                            logger.info('done judging, ALL GOOD')\n",
    "                            break\n",
    "\n",
    "                    extracted_data[data_part].update(table)\n",
    "\n",
    "        with (page_dir / \"corrected_extraction.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(extracted_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return page_idx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71a25ff8-a679-430e-b080-82bfcfa3f8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.53 s\n",
      "Wall time: 1h 32min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":  # critical for multiprocessing on Windows/macOS\n",
    "    # Extract PIL image, full text, bytes, base64 image from a PDF file\n",
    "    # pdf_extractor = PDFTextAndImageExtractor()\n",
    "    # per_page_text_and_images = pdf_extractor.process_pdf('obfuscated_fake_cbiz_prof_10_pages.pdf')\n",
    "\n",
    "    mgr = ExtractionLogManager(base_dir=Path(\"./runs\"),\n",
    "                               parent_logger_name=\"extract\",\n",
    "                               level=logging.INFO,\n",
    "                               enable_console=True)\n",
    "\n",
    "    # Create a unique extraction root\n",
    "    extraction_id, root = mgr.create_extraction_root()\n",
    "\n",
    "    judge_retry_max_attempts = 3\n",
    "\n",
    "    # Build tasks: keep only the fields the worker needs to reduce pickling overhead\n",
    "    tasks = []\n",
    "    for idx1, pg in enumerate(per_page_text_and_images, start=1):\n",
    "        tasks.append((\n",
    "            idx1,\n",
    "            {\n",
    "                \"page_bytes\": pg[\"page_bytes\"],\n",
    "                \"full_text\": pg[\"full_text\"],\n",
    "                \"base64_image\": pg[\"base64_image\"],\n",
    "            },\n",
    "            extraction_id,\n",
    "            root,\n",
    "            judge_retry_max_attempts,\n",
    "        ))\n",
    "\n",
    "    # Parallel execution (process-based via loky backend)\n",
    "    Parallel(n_jobs=2, backend=\"loky\")(\n",
    "        delayed(process_one_page)(*t) for t in tasks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e03f2301-309c-4c65-9720-a24480c990b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "extraction_path = r'D:\\ADP\\Data_Extraction\\data_extraction\\src\\runs\\\\' + extraction_id\n",
    "base = Path(extraction_path)\n",
    "\n",
    "for idx, page_dir in enumerate(os.listdir(base)):    \n",
    "    out_path = Path(base / page_dir / 'full_text.md')\n",
    "    \n",
    "    out_path.write_text(per_page_text_and_images[idx]['full_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
